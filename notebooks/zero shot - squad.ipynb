{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe20144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145c3c8e22794f318a95060c70254efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "from fp_dataset_artifacts.utils import init_openai\n",
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "\n",
    "init_openai()\n",
    "\n",
    "data = load_dataset('squad')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9da6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad853d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt without any structure or introduction\n",
    "def get_bare_prompt(x):\n",
    "    return f\"{x['context']}\\n\\n{x['question']}\\n\\n\"\n",
    "\n",
    "prompt = get_bare_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea4ca28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BU8BWV7dO9jJIL3hAU7cL29mvJpE at 0x7f94ef4f13b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638642779,\n",
       "  \"id\": \"cmpl-4BU8BWV7dO9jJIL3hAU7cL29mvJpE\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e433fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# Prompt with Q&A structure, but no introduction\n",
    "def get_QA_prompt(x):\n",
    "    return f\"{x['context']}\\n\\nQ: {x['question']}\\n\\nA: \"\n",
    "\n",
    "prompt = get_QA_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2655e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BU9gHy90Kg9APkKrBMAnQhDAIceU at 0x7f954f65be50> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638642872,\n",
       "  \"id\": \"cmpl-4BU9gHy90Kg9APkKrBMAnQhDAIceU\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec7c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Prompt with better Q&A structure, but no introduction\n",
    "def get_QA_prompt(x):\n",
    "    return f\"Context: {x['context']}\\n\\nQuestion: {x['question']}\\n\\nAnswer: \"\n",
    "\n",
    "prompt = get_QA_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad828976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BUAc2lGCyenaUdETW20ScnkZNRfz at 0x7f94efe76950> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638642930,\n",
       "  \"id\": \"cmpl-4BUAc2lGCyenaUdETW20ScnkZNRfz\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc8e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer.\n",
      "\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# All of the above and with the OpenAI's Q&A bot introduction without \"Unknown\" since it's 1.1\n",
    "intro = 'I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer.'\n",
    "\n",
    "def get_bot_prompt(x):\n",
    "    return f\"{intro}\\n\\nContext: {x['context']}\\n\\nQuestion: {x['question']}\\n\\nAnswer: \"\n",
    "\n",
    "prompt = get_bot_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96517437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BUDL0oIs66fbRBRPC66oZhjniuDV at 0x7f94efd3b270> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"_________\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638643099,\n",
       "  \"id\": \"cmpl-4BUDL0oIs66fbRBRPC66oZhjniuDV\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ae8e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are finally getting something from the model, but it's pretty much generating invalid tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2effb997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer.\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# To reduce the burden of large context, let's try it without it.\n",
    "def get_bot_no_context_prompt(x):\n",
    "    return f\"{intro}\\n\\nQuestion: {x['question']}\\n\\nAnswer: \"\n",
    "\n",
    "prompt = get_bot_no_context_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a198e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BUFpOZw2WR4FPqI0BGQgL0pZnuI2 at 0x7f954c5748b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"__________\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638643253,\n",
       "  \"id\": \"cmpl-4BUFpOZw2WR4FPqI0BGQgL0pZnuI2\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa40353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# Let's try even more compact.\n",
    "short_intro = 'I am a highly intelligent question answering bot.'\n",
    "\n",
    "def get_bot_no_context_prompt(x):\n",
    "    return f\"{short_intro}\\n\\nQ: {x['question']}\\n\\nA: \"\n",
    "\n",
    "prompt = get_bot_no_context_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f3f7506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BUHGQbFh0qiJCpCrezQC3eijo8aV at 0x7f94efd4d680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"__________\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638643342,\n",
       "  \"id\": \"cmpl-4BUHGQbFh0qiJCpCrezQC3eijo8aV\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79dd2203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: What countries are recognized as Nuclear Weapons States?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# Let's try a few different questions\n",
    "prompt = get_bot_no_context_prompt(data['train'].shuffle()[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e69d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4BUIveEkFHj2msuv4Nx0jAYzhrxTj at 0x7f94efd4def0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1638643445,\n",
       "  \"id\": \"cmpl-4BUIveEkFHj2msuv4Nx0jAYzhrxTj\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18f590a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: Who compared transnational police information and intelligence sharing practices?\n",
      "\n",
      "A: \n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"????\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638643499,\n",
      "  \"id\": \"cmpl-4BUJnhYRhNhLx0mVpKXmWK1t9ljVC\",\n",
      "  \"model\": \"curie:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Attempt 2\n",
    "prompt = get_bot_no_context_prompt(data['train'].shuffle()[1])\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc9a6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: What was the inspiration behind Kanye West's decision to call Rick Rubin?\n",
      "\n",
      "A: \n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"????\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638643502,\n",
      "  \"id\": \"cmpl-4BUJqQ7VrrrL75F4rUtsTee7LskfG\",\n",
      "  \"model\": \"curie:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Attempt 3\n",
    "prompt = get_bot_no_context_prompt(data['train'].shuffle()[2])\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd2f3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: On what part of the island is Fort Oscar located on the far side of?\n",
      "\n",
      "A: \n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"????\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638643518,\n",
      "  \"id\": \"cmpl-4BUK63QKDQfLtCTx5F4uTxMPd0jN0\",\n",
      "  \"model\": \"curie:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Attempt 4\n",
    "prompt = get_bot_no_context_prompt(data['train'].shuffle()[3])\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "  engine='curie',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3934beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Q: On what part of the island is Fort Oscar located on the far side of?\n",
      "\n",
      "A: \n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\u0caa\\u0ccd\\u0cb0\\u0cbf\\u0caf\\u0cae\\u0ccd\\u0cae\\u0ca8\\u0ccd \\u0cae\\u0ca8\\u0cc6\\u0caf\\u0cbf\\u0c82\\u0ca6 \\u0cae\\u0cc1\\u0c82\\u0ca6\\u0cc1\\u0cb5\\u0cb0\\u0cc6\\u0caf\\u0cb2\\u0ccd\\u0cb2\\u0cbf \\u0c85\\u0cb5\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638643551,\n",
      "  \"id\": \"cmpl-4BUKdWtcH70lHKUKnrCaG8DOLDS7C\",\n",
      "  \"model\": \"davinci:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Attempt 5 with Davinci\n",
    "prompt = get_bot_no_context_prompt(data['train'].shuffle()[3])\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "  engine='davinci',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "917719de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no point in trying to evaluate the entire dataset, if we can't get a single one to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fc872b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot.\n",
      "\n",
      "Among the notable structures in the town are the three forts built by the Swedes for defense purposes. One of these forts, known as Fort Oscar (formerly Gustav Adolph), which overlooks the sea is located on the far side of La Pointe. However, the ruins have been replaced by a modern military building which now houses the local gendarmerie. The other fort known as Fort Karl now presents a very few ruins. The third fort built by the Swedes is the Fort Gustav, which is also seen in ruins strewn around the weather station and the Light House. The fort built in 1787 over a hill slope has ruins of ramparts, guardhouse, munitions depot, wood-burning oven and so forth.\n",
      "\n",
      "Q: On what part of the island is Fort Oscar located on the far side of?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# One last try with everything we got\n",
    "def get_bot_context_prompt(x):\n",
    "    return f\"{short_intro}\\n\\n{x['context']}\\n\\nQ: {x['question']}\\n\\nA: \"\n",
    "\n",
    "prompt = get_bot_context_prompt(data['train'].shuffle()[3])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07772cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638644876,\n",
      "  \"id\": \"cmpl-4BUg0VB2WGLKlQLW0XbRpkGLZET7B\",\n",
      "  \"model\": \"davinci:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='davinci',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97efeea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-2bd8e69c8e2e88c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer.\n",
      "\n",
      "Among the notable structures in the town are the three forts built by the Swedes for defense purposes. One of these forts, known as Fort Oscar (formerly Gustav Adolph), which overlooks the sea is located on the far side of La Pointe. However, the ruins have been replaced by a modern military building which now houses the local gendarmerie. The other fort known as Fort Karl now presents a very few ruins. The third fort built by the Swedes is the Fort Gustav, which is also seen in ruins strewn around the weather station and the Light House. The fort built in 1787 over a hill slope has ruins of ramparts, guardhouse, munitions depot, wood-burning oven and so forth.\n",
      "\n",
      "Q: On what part of the island is Fort Oscar located on the far side of?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "def get_bot_context_prompt(x):\n",
    "    return f\"{intro}\\n\\n{x['context']}\\n\\nQ: {x['question']}\\n\\nA: \"\n",
    "\n",
    "prompt = get_bot_context_prompt(data['train'].shuffle()[3])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b715f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1638644930,\n",
      "  \"id\": \"cmpl-4BUgspdd9dXR90oeXvMXDKojt8YPS\",\n",
      "  \"model\": \"davinci:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine='davinci',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c4918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
